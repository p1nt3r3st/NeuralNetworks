{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c124f1b-9127-47c3-b0c9-368ec79f0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c548ee-af40-4cac-a490-394f84f3a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5431907-4357-4bc5-af59-7a59a1d87da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('archive/tasks_1-20_v1-2/en/qa1_single-supporting-fact_test.txt') as f:  # your file.txt. I had a text file from spermwhale dataset\n",
    "    raw = f.readlines()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw:\n",
    "    tokens.append(line.lower().replace('\\n', '').split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8db06b-1a67-4f5d-96ed-e8b4565c140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39f7fa-2680-49d9-afb4-34a40b89e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac63760-76bd-41ba-8400-8f8acbaeb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2indeces(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4904b-a9f8-4482-9c30-4231f0006177",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "embed_size = 10 # заданное кол-во нейронов\n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1 # (матрица весов для каждого слова)\n",
    "\n",
    "recurrent = np.eye(embed_size) # наша рекуррентная матрица(изначально - единичная)\n",
    "\n",
    "start = np.zeros(embed_size) # массив нужен для начального этапа прогноза и обучения\n",
    "\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1 # прогнозирующая матрица\n",
    "\n",
    "one_hot = np.eye(len(vocab)) # матрица, использующаяся как указание учителя при обратном распространении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99226339-957a-44be-9005-6859bd3274e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "    loss = 0\n",
    "\n",
    "    for target_i in range(len(sent)):\n",
    "\n",
    "        layer = {}\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456c745-5321-4087-8d5c-005fe78f5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "\n",
    "for iter in range(len(tokens) * 10):\n",
    "    sent = words2indeces(tokens[iter % len(tokens)])\n",
    "    layers, loss = predict(sent)\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "   \n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx - 1]\n",
    "\n",
    "        if layer_idx > 0:\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            if layer_idx == len(layers) - 1:\n",
    "                layer['hidden_delta'] = new_hidden_delta \n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else:\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n",
    "    start -= layers[0]['hidden_delta'] * alpha / len(sent)\n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / len(sent)\n",
    "\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / len(sent)\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / len(sent)\n",
    "    if iter % 10 == 0:\n",
    "        print(f'Итерация: {iter}')\n",
    "        print(f'Perplexity (Затруднение): {str(np.exp(loss / len(sent)))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
