{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3b22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e672fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, autograd=False, creators=None, creation_op=None, idr=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if idr is None:\n",
    "            idr = np.random.randint(0, 100000)\n",
    "        self.idr = idr\n",
    "        \n",
    "        if creators:\n",
    "            for c in creators:\n",
    "                c.children[idr] = c.children.get(idr, 0) + 1\n",
    "                \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for idr, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        \n",
    "        if self.autograd:\n",
    "            if grad_origin:\n",
    "                if self.children[grad_origin.idr] == 0:\n",
    "                    raise Exception(\"can't backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.idr] -= 1\n",
    "                    \n",
    "            if not grad:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            if self.grad:\n",
    "                self.grad += grad\n",
    "            else:\n",
    "                self.grad = grad\n",
    "                \n",
    "            if self.creators and (self.all_children_grads_accounted_for() or not grad_origin):\n",
    "                if self.creation_op == 'add':\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if self.creation_op == 'neg':\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if self.creation_op == 'sub':\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if self.creation_op == 'mul':\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if self.creation_op == 'mm':\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                    \n",
    "                if self.creation_op == 'transpose':\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                \n",
    "                if self.creation_op.startswith('sum'):\n",
    "                    dim = int(self.creation_op.split('_')[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "                \n",
    "                if self.creation_op.startswith('expand'):\n",
    "                    dim = int(self.creation_op.split('_')[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if self.creation_op == 'sigmoid':\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                    \n",
    "                if self.creation_op == 'softmax':\n",
    "                    self.creators[0].backward(self.grad * self)\n",
    "                    \n",
    "                if self.creation_op == 'tanh':\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                    \n",
    "                if self.creation_op == 'index_select':\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if self.creation_op == 'cross_entropy':\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "        \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op='add')\n",
    "        else:\n",
    "            return Tensor(self.data + other.data)\n",
    "        \n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='neg')\n",
    "        else:\n",
    "            return Tensor(self.data * -1)\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op='sub')\n",
    "        else:\n",
    "            return Tensor(self.data - other.data)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op='mul')\n",
    "        else:\n",
    "            return Tensor(self.data * other.data)\n",
    "        \n",
    "    def sum(self, dim):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='sum_' + str(dim))\n",
    "        else:\n",
    "            return Tensor(self.data.sum(dim))\n",
    "        \n",
    "    def expand(self, dim, copies):\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='expand_' + str(dim))\n",
    "        else:\n",
    "            return Tensor(new_data)\n",
    "        \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='transpose')\n",
    "        else:\n",
    "            return Tensor(self.data.transpose())\n",
    "        \n",
    "    def mm(self, x):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                         autograd=True,\n",
    "                         creators=[self, x],\n",
    "                         creation_op='mm')\n",
    "        else:\n",
    "            return Tensor(self.data.dot(x.data))\n",
    "        \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='sigmoid')\n",
    "        else:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "        \n",
    "    def softmax(self):\n",
    "        if self.autograd:\n",
    "            temp = np.exp(self.data)\n",
    "            softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "            return Tensor(softmax_output,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='softmax')\n",
    "        else:\n",
    "            return Tensor(softmax_output)\n",
    "        \n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='tanh')\n",
    "        else:\n",
    "            return Tensor(np.tanh(self.data))\n",
    "        \n",
    "    def index_select(self, indices):\n",
    "        if self.autograd:\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                        autograd=True,\n",
    "                        creators=[self],\n",
    "                        creation_op='index_select')\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        else:\n",
    "            Tensor(self.data[indices.data])\n",
    "            \n",
    "    def cross_entropy(self, target_indices):\n",
    "        \n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1,\n",
    "                                      keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        if self.autograd:\n",
    "            out = Tensor(loss,\n",
    "                        autograd=True,\n",
    "                        creators=[self],\n",
    "                        creation_op='cross_entropy')\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        else:\n",
    "            return Tensor(loss)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ce5337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "    \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if zero:\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d65a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16bfe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2 / n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return inputs.mm(self.weight) + self.bias.expand(0, len(inputs.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23150b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "004a005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target) * (pred - target)).sum(0)\n",
    "        # return ((pred - target) * (pred - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c94109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return inputs.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795cbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return inputs.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7019d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return inputs.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f74ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.weight.index_select(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688807c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init_(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        return inputs.cross_entropy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "544f9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception('Non-linearity not found')\n",
    "            \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(inputs) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e36641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 4 2 2]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
    "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
    "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
    "g = Tensor([6, 7, 8, 9, 10], autograd=True)\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor([1, 1, 2, 1, 1]))\n",
    "\n",
    "\n",
    "print(b.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83a643e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.10217643]\n",
      "1 [1.81873036]\n",
      "2 [1.66450385]\n",
      "3 [1.3043611]\n",
      "4 [0.70899744]\n",
      "5 [0.52007822]\n",
      "6 [0.36230166]\n",
      "7 [0.24114239]\n",
      "8 [0.16432991]\n",
      "9 [0.1185035]\n",
      "10 [0.0904326]\n",
      "11 [0.07209722]\n",
      "12 [0.05940884]\n",
      "13 [0.05021683]\n",
      "14 [0.04330444]\n",
      "15 [0.03794565]\n",
      "16 [0.03368593]\n",
      "17 [0.03022856]\n",
      "18 [0.02737273]\n",
      "19 [0.02497827]\n",
      "20 [0.02294467]\n",
      "21 [0.02119814]\n",
      "22 [0.01968344]\n",
      "23 [0.01835841]\n",
      "24 [0.01719038]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), autograd=True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2, 3), Tanh(), Linear(3, 1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "        \n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(25):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88313047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.09345749]\n",
      "1 [0.66336982]\n",
      "2 [0.41780476]\n",
      "3 [0.26952761]\n",
      "4 [0.18308312]\n",
      "5 [0.13210084]\n",
      "6 [0.10047446]\n",
      "7 [0.07968441]\n",
      "8 [0.06528621]\n",
      "9 [0.05486944]\n",
      "10 [0.04705724]\n",
      "11 [0.04102219]\n",
      "12 [0.03624388]\n",
      "13 [0.03238165]\n",
      "14 [0.02920475]\n",
      "15 [0.02655208]\n",
      "16 [0.02430822]\n",
      "17 [0.0223886]\n",
      "18 [0.02072997]\n",
      "19 [0.01928419]\n",
      "20 [0.01801404]\n",
      "21 [0.01689034]\n",
      "22 [0.0158899]\n",
      "23 [0.01499409]\n",
      "24 [0.0141878]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([1, 2, 1, 2]), autograd=True)\n",
    "target = Tensor(np.array([[0],\n",
    "                         [1],\n",
    "                         [0],\n",
    "                         [1]]), autograd=True)\n",
    "\n",
    "embed = Embedding(7, 3)\n",
    "model = Sequential([embed, Tanh(), Linear(3, 1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.5)\n",
    "\n",
    "for i in range(25):\n",
    "    pred = model.forward(data)  # прямое распространение\n",
    "    loss = criterion.forward(pred, target)  # находим ошибку\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))  # вычисляем градиенты\n",
    "    optim.step()  # Корректируем  веса\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c869f60b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.3565187521342192\n",
      "2 0.975111452377638\n",
      "3 0.722617821915376\n",
      "4 0.5555894243377575\n",
      "5 0.44137355129219086\n",
      "6 0.3603557230372992\n",
      "7 0.3009934976114045\n",
      "8 0.25626112058327233\n",
      "9 0.22171851539026366\n",
      "10 0.19446664919401047\n",
      "11 0.17255816981435335\n",
      "12 0.15465133624915467\n",
      "13 0.13980008380954706\n",
      "14 0.12732312061664763\n",
      "15 0.11672027792231379\n",
      "16 0.10761783784357454\n",
      "17 0.09973204253631165\n",
      "18 0.09284425664294993\n",
      "19 0.08678374974905959\n",
      "20 0.08141555616777205\n",
      "21 0.07663177816562065\n",
      "22 0.07234526356945385\n",
      "23 0.06848494621256507\n",
      "24 0.06499236794421728\n",
      "25 0.061819051700820304\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([1, 2, 1, 2]), autograd=True)\n",
    "target = Tensor(np.array([0, 1, 0, 1]), autograd=True)\n",
    "model = Sequential([Embedding(3, 3), Tanh(), Linear(3, 4)])\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(25):\n",
    "    pred = model.forward(data) \n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(i + 1, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc915ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
